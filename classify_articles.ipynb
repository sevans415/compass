{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# File that uses our best Naive Bayes model to classify full articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dat = pd.read_excel('all_sentences.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_preds(probs, thresh):\n",
    "    y_pred = []\n",
    "    for row in probs:\n",
    "        if row[1] > thresh:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(np.argmax(row))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55024813895781632"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best model\n",
    "X_train, X_test, y_train, y_test = train_test_split(dat['words_clean'], dat['label'], test_size=0.3)\n",
    "nb_model = Pipeline([('vect', CountVectorizer(ngram_range=(1,1))), \n",
    "                     ('tfidf', TfidfTransformer()), ('clf', MultinomialNB()),])\n",
    "nb_model.fit(X_train, y_train)\n",
    "probs = nb_model.predict_proba(X_test)\n",
    "return_acc(probs, y_test, 0.17)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to clean word data--removes stopwords, makes lowercase, removes numbers\n",
    "def clean_words(sentences):\n",
    "    words_clean = np.full(len(sentences), None)\n",
    "    for i, words in enumerate(sentences):\n",
    "        word_list = re.split('\\W+', words)\n",
    "        words1 = [word.lower() for word in word_list if word.lower() not in stopwords.words('english')]\n",
    "        words2 = [word for word in words1 if not any(char.isdigit() for char in word)]\n",
    "        words_clean[i] = (' '.join(words2)).strip()\n",
    "    return words_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to check if article has text (blank articles sometimes appear)\n",
    "def has_content(message):\n",
    "    return message != ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Score articles by taking the mean of the difference in probabilities of lib and con for each sentences\n",
    "def score_article_unsupervised(fname):\n",
    "    f = open(fname,'r')\n",
    "    message = f.read()\n",
    "    #print(message)\n",
    "    f.close()\n",
    "    if has_content(message):\n",
    "        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        fp = open(fname)\n",
    "        data = fp.read()\n",
    "        #print('\\n-----\\n'.join(tokenizer.tokenize(data)))\n",
    "        sentences = tokenizer.tokenize(data)\n",
    "        new_text = clean_words(sentences)\n",
    "\n",
    "        total = nb_model.predict(new_text)\n",
    "        probs = nb_model.predict_proba(new_text)\n",
    "        diffs = [row[2]-row[0] for row in probs]\n",
    "\n",
    "        return probs, np.mean(diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Score each outlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 368, 12\n",
    "fox_news = ['arts/fox/fox' + str(i) + '.txt' for i in range(368)]\n",
    "fox_scores = []\n",
    "for i in range(len(fox_news)):\n",
    "    if score_article_unsupervised(fox_news[i]) != None:\n",
    "        fox_scores.append(score_article_unsupervised(fox_news[i])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 320, 23\n",
    "nyt_news = ['arts/nyt/nyt' + str(i) + '.txt' for i in range(320)]\n",
    "nyt_scores = []\n",
    "for i in range(len(nyt_news)):\n",
    "    if score_article_unsupervised(nyt_news[i]) != None:\n",
    "        nyt_scores.append(score_article_unsupervised(nyt_news[i])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 823, 57\n",
    "cnn_news = ['arts/cnn/cnn' + str(i) + '.txt' for i in range(823)]\n",
    "cnn_scores = []\n",
    "for i in range(len(cnn_news)):\n",
    "    if score_article_unsupervised(cnn_news[i]) != None:\n",
    "        cnn_scores.append(score_article_unsupervised(cnn_news[i])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.108170465399\n",
      "-0.133872863003\n",
      "-0.118018135553\n"
     ]
    }
   ],
   "source": [
    "# Overall average political score\n",
    "print(np.mean(fox_scores))\n",
    "print(np.mean(nyt_scores))\n",
    "print(np.mean(cnn_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=-4.0775755512548564, pvalue=4.8710911694821782e-05)\n",
      "Ttest_indResult(statistic=-2.3241519564134054, pvalue=0.020296171422587401)\n",
      "Ttest_indResult(statistic=5.2072646749596316, pvalue=2.6412148897569954e-07)\n"
     ]
    }
   ],
   "source": [
    "# Check for significant difference in means\n",
    "print stats.ttest_ind(nyt_scores, cnn_scores)\n",
    "print stats.ttest_ind(cnn_scores, fox_scores)\n",
    "print stats.ttest_ind(fox_scores, nyt_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test on pretrained data (labeling done by making assumptions about outlet)\n",
    "# BAD supervised model--not statistically sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_article_supervised(fname):\n",
    "    f = open(fname,'r')\n",
    "    message = f.read()\n",
    "    #print(message)\n",
    "    f.close()\n",
    "    #import nltk.data\n",
    "    if has_content(message):\n",
    "        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        fp = open(fname)\n",
    "        data = fp.read()\n",
    "        #print('\\n-----\\n'.join(tokenizer.tokenize(data)))\n",
    "        sentences = tokenizer.tokenize(data)\n",
    "        new_text = clean_words(sentences)\n",
    "\n",
    "        total = nb_model.predict(new_text)\n",
    "        probs = nb_model.predict_proba(new_text)\n",
    "        #diffs = [row[2]-row[0] for row in probs]\n",
    "        lib = sum([row[0] for row in probs])/len(probs)\n",
    "        neut = sum([row[1] for row in probs])/len(probs)\n",
    "        con = sum([row[2] for row in probs])/len(probs)\n",
    "        \n",
    "        #print(probs)\n",
    "        #print(type(y_pred), type(y_test))\n",
    "        #print(np.mean(y_pred == y_test))\n",
    "        #print(y_pred)\n",
    "        #if y_pred == []:\n",
    "        #    return None\n",
    "        #print(y_pred)\n",
    "        #print(y_test.tolist())\n",
    "        #print(probs)\n",
    "        #print(y_pred)\n",
    "        return [lib, neut, con]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "breit = ['opinion_articles/breit/breit' + str(i) + '.txt' for i in range(38)]\n",
    "cnn = ['opinion_articles/cnn/cnn' + str(i) + '.txt' for i in range(38)]\n",
    "fox = ['opinion_articles/fox/fox' + str(i) + '.txt' for i in range(39)]\n",
    "new_yorker = ['opinion_articles/new_yorker/new_yorker' + str(i) + '.txt' for i in range(39)]\n",
    "nyt = ['opinion_articles/nyt/nyt' + str(i) + '.txt' for i in range(38)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sources = ['breit', 'fox', 'new_yorker', 'nyt', 'cnn']\n",
    "nums = [38, 39, 39, 38, 38]\n",
    "labels = [0.7, 0.3, -0.5, -0.3, -0.1]\n",
    "\n",
    "def make_fname(source, num, label, all_articles):\n",
    "    arts = [('opinion_articles/' + source + '/' + source + str(i) + '.txt', label) for i in range(num)]\n",
    "    all_articles += arts\n",
    "\n",
    "articles = []    \n",
    "for tup in zip(sources, nums, labels):\n",
    "    make_fname(tup[0], tup[1], tup[2], articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'opinion_articles/breit/breit1.txt'"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lib_scores = np.full(len(articles), None)\n",
    "neut_scores = np.full(len(articles), None)\n",
    "con_scores = np.full(len(articles), None)\n",
    "labels = np.full(len(articles), None)\n",
    "for i, article in enumerate(articles):\n",
    "    ret = score_article_supervised(articles[i][0])\n",
    "    lib_scores[i] = ret[0]\n",
    "    neut_scores[i] = ret[1]\n",
    "    con_scores[i] = ret[2]\n",
    "    labels[i] = (articles[i][1])#/2 + 0.5\n",
    "    \n",
    "articles_supervised = pd.DataFrame(data={'lib':lib_scores, 'neut':neut_scores,\n",
    "                                          'con':con_scores, 'label':labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>con</th>\n",
       "      <th>label</th>\n",
       "      <th>lib</th>\n",
       "      <th>neut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.369839</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.378079</td>\n",
       "      <td>0.252082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.227844</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.350113</td>\n",
       "      <td>0.422043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.39259</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4145</td>\n",
       "      <td>0.19291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.313</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.464913</td>\n",
       "      <td>0.222087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.319412</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.356282</td>\n",
       "      <td>0.324306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.310305</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.321217</td>\n",
       "      <td>0.368478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.295338</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.388957</td>\n",
       "      <td>0.315706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.359949</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.438573</td>\n",
       "      <td>0.201479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.293797</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.485502</td>\n",
       "      <td>0.220701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.432443</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.396859</td>\n",
       "      <td>0.170697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.299083</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.486087</td>\n",
       "      <td>0.21483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.314959</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.468276</td>\n",
       "      <td>0.216765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.323691</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.419472</td>\n",
       "      <td>0.256838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.351155</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.510242</td>\n",
       "      <td>0.138603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.30711</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.531891</td>\n",
       "      <td>0.160999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.355032</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.412249</td>\n",
       "      <td>0.232719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.331525</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.445125</td>\n",
       "      <td>0.223349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.287644</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.621689</td>\n",
       "      <td>0.0906675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.351931</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.40802</td>\n",
       "      <td>0.240049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.351024</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.451752</td>\n",
       "      <td>0.197224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.342252</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.442392</td>\n",
       "      <td>0.215357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.289007</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.356373</td>\n",
       "      <td>0.354619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.340311</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.453766</td>\n",
       "      <td>0.205923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.289805</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.331048</td>\n",
       "      <td>0.379147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.307319</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.430023</td>\n",
       "      <td>0.262658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.386393</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.411261</td>\n",
       "      <td>0.202346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.282241</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.337804</td>\n",
       "      <td>0.379955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.297939</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.454338</td>\n",
       "      <td>0.247723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.347649</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.480639</td>\n",
       "      <td>0.171712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.305934</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.448462</td>\n",
       "      <td>0.245604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.276619</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.499315</td>\n",
       "      <td>0.224066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.283832</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.551479</td>\n",
       "      <td>0.16469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.350933</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.328194</td>\n",
       "      <td>0.320873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0.272218</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.36136</td>\n",
       "      <td>0.366421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.309124</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.498458</td>\n",
       "      <td>0.192418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0.336847</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.462638</td>\n",
       "      <td>0.200515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.344318</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.428441</td>\n",
       "      <td>0.227241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.342002</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.452251</td>\n",
       "      <td>0.205747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0.351226</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.443096</td>\n",
       "      <td>0.205677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.311709</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.443714</td>\n",
       "      <td>0.244577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.302616</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.375775</td>\n",
       "      <td>0.32161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.363455</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.461643</td>\n",
       "      <td>0.174902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.283</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.574141</td>\n",
       "      <td>0.142859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.320833</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.465042</td>\n",
       "      <td>0.214124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.337342</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.414735</td>\n",
       "      <td>0.247923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.337735</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.513355</td>\n",
       "      <td>0.14891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.319854</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.451304</td>\n",
       "      <td>0.228842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.316097</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.414876</td>\n",
       "      <td>0.269027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.291467</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.55119</td>\n",
       "      <td>0.157343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.332242</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.436513</td>\n",
       "      <td>0.231245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.283832</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.551479</td>\n",
       "      <td>0.16469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.313124</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.539947</td>\n",
       "      <td>0.146929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.31547</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.489933</td>\n",
       "      <td>0.194597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.339145</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.358904</td>\n",
       "      <td>0.301951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.268628</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.382721</td>\n",
       "      <td>0.348651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.289871</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.588638</td>\n",
       "      <td>0.121492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.359362</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.479932</td>\n",
       "      <td>0.160706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.322501</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.47514</td>\n",
       "      <td>0.202359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.346047</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.434432</td>\n",
       "      <td>0.219522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.329026</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.447992</td>\n",
       "      <td>0.222983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          con label       lib       neut\n",
       "0    0.369839   0.7  0.378079   0.252082\n",
       "1    0.227844   0.7  0.350113   0.422043\n",
       "2     0.39259   0.7    0.4145    0.19291\n",
       "3       0.313   0.7  0.464913   0.222087\n",
       "4    0.319412   0.7  0.356282   0.324306\n",
       "5    0.310305   0.7  0.321217   0.368478\n",
       "6    0.295338   0.7  0.388957   0.315706\n",
       "7    0.359949   0.7  0.438573   0.201479\n",
       "8    0.293797   0.7  0.485502   0.220701\n",
       "9    0.432443   0.7  0.396859   0.170697\n",
       "10   0.299083   0.7  0.486087    0.21483\n",
       "11   0.314959   0.7  0.468276   0.216765\n",
       "12   0.323691   0.7  0.419472   0.256838\n",
       "13   0.351155   0.7  0.510242   0.138603\n",
       "14    0.30711   0.7  0.531891   0.160999\n",
       "15   0.355032   0.7  0.412249   0.232719\n",
       "16   0.331525   0.7  0.445125   0.223349\n",
       "17   0.287644   0.7  0.621689  0.0906675\n",
       "18   0.351931   0.7   0.40802   0.240049\n",
       "19   0.351024   0.7  0.451752   0.197224\n",
       "20   0.342252   0.7  0.442392   0.215357\n",
       "21   0.289007   0.7  0.356373   0.354619\n",
       "22   0.340311   0.7  0.453766   0.205923\n",
       "23   0.289805   0.7  0.331048   0.379147\n",
       "24   0.307319   0.7  0.430023   0.262658\n",
       "25   0.386393   0.7  0.411261   0.202346\n",
       "26   0.282241   0.7  0.337804   0.379955\n",
       "27   0.297939   0.7  0.454338   0.247723\n",
       "28   0.347649   0.7  0.480639   0.171712\n",
       "29   0.305934   0.7  0.448462   0.245604\n",
       "..        ...   ...       ...        ...\n",
       "162  0.276619  -0.1  0.499315   0.224066\n",
       "163  0.283832  -0.1  0.551479    0.16469\n",
       "164  0.350933  -0.1  0.328194   0.320873\n",
       "165  0.272218  -0.1   0.36136   0.366421\n",
       "166  0.309124  -0.1  0.498458   0.192418\n",
       "167  0.336847  -0.1  0.462638   0.200515\n",
       "168  0.344318  -0.1  0.428441   0.227241\n",
       "169  0.342002  -0.1  0.452251   0.205747\n",
       "170  0.351226  -0.1  0.443096   0.205677\n",
       "171  0.311709  -0.1  0.443714   0.244577\n",
       "172  0.302616  -0.1  0.375775    0.32161\n",
       "173  0.363455  -0.1  0.461643   0.174902\n",
       "174     0.283  -0.1  0.574141   0.142859\n",
       "175  0.320833  -0.1  0.465042   0.214124\n",
       "176  0.337342  -0.1  0.414735   0.247923\n",
       "177  0.337735  -0.1  0.513355    0.14891\n",
       "178  0.319854  -0.1  0.451304   0.228842\n",
       "179  0.316097  -0.1  0.414876   0.269027\n",
       "180  0.291467  -0.1   0.55119   0.157343\n",
       "181  0.332242  -0.1  0.436513   0.231245\n",
       "182  0.283832  -0.1  0.551479    0.16469\n",
       "183  0.313124  -0.1  0.539947   0.146929\n",
       "184   0.31547  -0.1  0.489933   0.194597\n",
       "185  0.339145  -0.1  0.358904   0.301951\n",
       "186  0.268628  -0.1  0.382721   0.348651\n",
       "187  0.289871  -0.1  0.588638   0.121492\n",
       "188  0.359362  -0.1  0.479932   0.160706\n",
       "189  0.322501  -0.1   0.47514   0.202359\n",
       "190  0.346047  -0.1  0.434432   0.219522\n",
       "191  0.329026  -0.1  0.447992   0.222983\n",
       "\n",
       "[192 rows x 4 columns]"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(articles_supervised.drop(['label'], axis=1), \n",
    "                                                    articles_supervised['label'], test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.frame.DataFrame, pandas.core.series.Series)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train), type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model = LinearRegression()\n",
    "log_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03125 ,  0.265625, -0.140625,  0.09375 ,  0.      ,  0.09375 ,\n",
       "        0.125   ,  0.046875, -0.09375 ,  0.15625 ,  0.046875,  0.015625,\n",
       "       -0.171875,  0.109375,  0.078125,  0.03125 ,  0.0625  , -0.15625 ,\n",
       "       -0.078125,  0.015625,  0.3125  , -0.171875, -0.171875, -0.140625,\n",
       "        0.015625, -0.046875, -0.15625 ,  0.15625 ,  0.265625, -0.03125 ,\n",
       "        0.140625,  0.1875  ,  0.125   , -0.046875,  0.171875, -0.03125 ,\n",
       "        0.03125 , -0.171875,  0.078125, -0.078125,  0.234375, -0.09375 ,\n",
       "        0.      , -0.1875  , -0.015625,  0.015625,  0.046875, -0.078125])"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001027253121313354"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17641581217447919"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
